{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML trained on every file, tested on one at a time (30 seconds, 4 cores)\n",
    "All models are untuned. The model will be trained on file A, tested on files B,C,D.., then on B, tested on A,C,D.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from modules.NetworkTraffic import NetworkTraffic\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [AutoSklearn2Classifier(time_left_for_this_task=300, n_jobs=-1, memory_limit=4096)]\n",
    "OutputResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, x_train, y_train):\n",
    "  #print(f\"Testing {str(model)}\", end=', ')\n",
    "  start = process_time()\n",
    "\n",
    "  ### Begin timing\n",
    "  temp_clf = model\n",
    "  temp_clf.fit(x_train, y_train)\n",
    "\n",
    "  ### End timing\n",
    "\n",
    "  stop = process_time()\n",
    "  return [temp_clf, (stop-start)]\n",
    "\n",
    "\n",
    "def testModel(model, x_test, y_test, runtime):\n",
    "  y_pred = model.predict(x_test)\n",
    "  # Results\n",
    "  tempDict = {\n",
    "    \"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "    \"Balanced Accuracy\": metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "    \"F1 Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Precision Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Recall Micro\": metrics.recall_score(y_test, y_pred, average='micro'),\n",
    "    \"Runtime\": runtime,\n",
    "  }\n",
    "  try:\n",
    "    tempDict[\"Final Ensemble\"] = model.show_models()\n",
    "  except KeyError:\n",
    "    tempDict[\"Final Ensemble\"] = None\n",
    "  try:\n",
    "    tempDict[\"Leaderboard\"] = str(model.leaderboard())\n",
    "  except:\n",
    "    pass\n",
    "  return {str(model): tempDict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileObjects = dict()\n",
    "\n",
    "for file in FilesToTest:\n",
    "  FileObjects[file] = NetworkTraffic(file, testSize=0.4, doNorm=True, doNormAll=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b5000d100.csv [b5000d30.csv b100d10.csv b1000d10.csv b1000d100.csv b100d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b5000d30.csv [b5000d100.csv b100d10.csv b1000d10.csv b1000d100.csv b100d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b100d10.csv [b5000d100.csv b5000d30.csv b1000d10.csv b1000d100.csv b100d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b1000d10.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d100.csv b100d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b1000d100.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d10.csv b100d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b100d100.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d10.csv b1000d100.csv b5000d10.csv b1000d30.csv b100d30.csv ]\n",
      "b5000d10.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d10.csv b1000d100.csv b100d100.csv b1000d30.csv b100d30.csv ]\n",
      "b1000d30.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d10.csv b1000d100.csv b100d100.csv b5000d10.csv b100d30.csv ]\n",
      "b100d30.csv [b5000d100.csv b5000d30.csv b100d10.csv b1000d10.csv b1000d100.csv b100d100.csv b5000d10.csv b1000d30.csv ]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "OutputResults.clear()\n",
    "\n",
    "for index, file in enumerate(FilesToTest):\n",
    "  print(file, end=' ')\n",
    "  OutputResults[file] = dict()\n",
    "  #currentFileData = NetworkTraffic(file, testSize=0.4, doNorm=True, doNormAll=True)\n",
    "  currentFileData = FileObjects[file]\n",
    "  restOfFiles = deepcopy(FilesToTest)\n",
    "  restOfFiles.pop(index)\n",
    "  api, runtime = trainModel(ModelsToTest[0], currentFileData.data, currentFileData.target)\n",
    "  print('[', end='')\n",
    "  for file2 in restOfFiles:\n",
    "    print(file2, end=' ')\n",
    "    OutputResults[file][file2] = dict()\n",
    "    #testFileData = NetworkTraffic(file2, testSize=0.4, doNorm=True, doNormAll=True)\n",
    "    testFileData = FileObjects[file2]\n",
    "    x_test, y_test = testFileData.data, testFileData.target\n",
    "    #print(f\"{file} : {str(model)}...\")\n",
    "    results = testModel(api, x_test, y_test, runtime)\n",
    "    OutputResults[file][file2] = results\n",
    "  print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyOfOutput = OutputResults\n",
    "for size in OutputResults:\n",
    "  for file in OutputResults[size]:\n",
    "    for model in OutputResults[size][file]:\n",
    "      for attribute in OutputResults[size][file][model]:\n",
    "        if type(OutputResults[size][file][model][attribute]) not in [str, int, float]:\n",
    "          copyOfOutput[size][file][model][attribute] = str(OutputResults[size][file][model][attribute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"EveryFileTransfer_Untuned_AllTestResults.json\", \"w\") as f:\n",
    "  f.write(json.dumps(OutputResults, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EveryFileTransfer_Untuned_ModelResults.csv\", \"w\") as f3:\n",
    "  f3.write(\"Trained On,Tested On,Model,Accuracy,Runtime\\n\")\n",
    "  for file in OutputResults:\n",
    "    for file2 in OutputResults[file]:\n",
    "      for model in OutputResults[file][file2]:\n",
    "        f3.write(f\"{file},{file2},{model.replace(',', '')},{OutputResults[file][file2][model]['Accuracy']},{OutputResults[file][file2][model]['Runtime']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
