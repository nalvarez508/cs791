{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML (askl2) trained on every file individually, tested on the rest (4 cores, 30 seconds per file)\n",
    "AutoML's data preprocessing is applied to these tests. The model will be trained on file A, tested on files B,C,D.., then on B, tested on A,C,D.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir\n",
    "from copy import deepcopy\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from modules.NetworkTraffic import NetworkTraffic\n",
    "from sklearn import model_selection, metrics\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [AutoSklearn2Classifier(time_left_for_this_task=30, memory_limit=4096, n_jobs=-1)]\n",
    "OutputResults = dict()\n",
    "#ModelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, x_train, x_test, y_train, y_test):\n",
    "  start = process_time()\n",
    "\n",
    "  ### Begin timing\n",
    "  temp_clf = model\n",
    "  temp_clf.fit(x_train, y_train)\n",
    "\n",
    "  y_pred = temp_clf.predict(x_test)\n",
    "  ### End timing\n",
    "\n",
    "  stop = process_time()\n",
    "\n",
    "  # Results\n",
    "  tempDict = {\n",
    "    \"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "    \"Balanced Accuracy\": metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "    \"F1 Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Precision Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Recall Micro\": metrics.recall_score(y_test, y_pred, average='micro'),\n",
    "    \"Runtime\": stop-start,\n",
    "  }\n",
    "  # try:\n",
    "  #   tempDict[\"Leaderboard\"] = str(temp_clf.leaderboard())\n",
    "  # except KeyError:\n",
    "  #   tempDict[\"Leaderboard\"] = None\n",
    "  try:\n",
    "    tempDict[\"Final Ensemble\"] = temp_clf.show_models()\n",
    "  except KeyError:\n",
    "    tempDict[\"Final Ensemble\"] = None\n",
    "  try:\n",
    "    tempDict[\"Leaderboard\"] = str(temp_clf.leaderboard())\n",
    "  except:\n",
    "    pass\n",
    "  return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModelResults(size, model, results):\n",
    "  def changeKeyValue():\n",
    "    try:\n",
    "      ModelResults[size][model][key] += results[key]\n",
    "    except KeyError:\n",
    "      ModelResults[size][model][key] = results[key]\n",
    "  \n",
    "  # For each metric, attempt to set or add to the value\n",
    "  for key in results:\n",
    "    if key not in [\"Leaderboard\", \"Final Ensemble\"]:\n",
    "      try:\n",
    "        changeKeyValue()\n",
    "      except KeyError:\n",
    "        ModelResults[size][model] = dict()\n",
    "        changeKeyValue()\n",
    "\n",
    "# Divide each metric by the total number of files tested\n",
    "def findAveragesForModelResults(fileCount):\n",
    "  for size in ModelResults:\n",
    "    for model in ModelResults[size]:\n",
    "      for metric in ModelResults[size][model]:\n",
    "        if metric not in [\"Leaderboard\", \"Final Ensemble\"]:\n",
    "          ModelResults[size][model][metric] /= fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching with test size of 40.0%...\n",
      "b5000d100.csv, b5000d100.csv : AutoSklearn2Classifier(memory_limit=4096, n_jobs=-1, time_left_for_this_task=30), b5000d30.csv, b5000d30.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b100d10.csv, b100d10.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b1000d10.csv, b1000d10.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b1000d100.csv, b1000d100.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b100d100.csv, b100d100.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b5000d10.csv, b5000d10.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b1000d30.csv, b1000d30.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), b100d30.csv, b100d30.csv : AutoSklearn2Classifier(memory_limit=4096, metric=accuracy, n_jobs=-1,\n",
      "                       per_run_time_limit=12, time_left_for_this_task=30), "
     ]
    }
   ],
   "source": [
    "OutputResults.clear()\n",
    "#ModelResults.clear()\n",
    "\n",
    "for size in TestSize:\n",
    "  print(f\"\\nSearching with test size of {size*100}%...\")\n",
    "  OutputResults[size] = dict()\n",
    "  #ModelResults[size] = dict()\n",
    "\n",
    "  for index, file in enumerate(FilesToTest):\n",
    "    print(file, end=', ')\n",
    "    OutputResults[size][file] = dict()\n",
    "    currentFileData = NetworkTraffic(file, testSize=size, doNorm=True, doNormAll=True)\n",
    "    restOfFiles = deepcopy(FilesToTest)\n",
    "    restOfFiles.pop(index)\n",
    "    restOfTheFilesData = NetworkTraffic(restOfFiles, testSize=size, doNorm=True, doNormAll=True)\n",
    "    x_train, y_train, x_test, y_test = currentFileData.data, currentFileData.target, restOfTheFilesData.data, restOfTheFilesData.target\n",
    "\n",
    "    for model in ModelsToTest:\n",
    "      print(f\"{file} : {str(model)}\", end=\", \")\n",
    "      results = testModel(model, x_train, x_test, y_train, y_test)\n",
    "      OutputResults[size][file].update({str(model): results})\n",
    "      #updateModelResults(size, str(model), results)\n",
    "\n",
    "#findAveragesForModelResults(len(FilesToTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyOfOutput = OutputResults\n",
    "for size in OutputResults:\n",
    "  for file in OutputResults[size]:\n",
    "    for model in OutputResults[size][file]:\n",
    "      for attribute in OutputResults[size][file][model]:\n",
    "        if type(OutputResults[size][file][model][attribute]) not in [str, int, float]:\n",
    "          copyOfOutput[size][file][model][attribute] = str(OutputResults[size][file][model][attribute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"EveryFileIndividually_Untuned_AllTestResults.json\", \"a\") as f:\n",
    "  f.write(json.dumps(copyOfOutput, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EveryFileTransfer_Untuned_ModelResults.csv\", \"w\") as f3:\n",
    "  f3.write(\"File Trained On,Model,Accuracy,Runtime\\n\")\n",
    "  for size in OutputResults:\n",
    "    for file in OutputResults[size]:\n",
    "      for model in OutputResults[size][file]:\n",
    "        f3.write(f\"{file},{model},{OutputResults[size][file][model]['Accuracy']},{OutputResults[size][file][model]['Runtime']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
