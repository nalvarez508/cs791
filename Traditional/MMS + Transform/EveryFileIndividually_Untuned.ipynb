{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods tested on every file individually (untuned)\n",
    "MinMaxScaler is applied to these tests. All models are untuned. Results from each model's tests are compiled into a single average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from modules.NetworkTraffic import NetworkTraffic\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [RandomForestClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(), MLPClassifier(), LinearSVC()]\n",
    "OutputResults = dict()\n",
    "ModelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, x_train, x_test, y_train, y_test):\n",
    "  start = process_time()\n",
    "\n",
    "  ### Begin timing\n",
    "  temp_clf = model\n",
    "  temp_clf.fit(x_train, y_train)\n",
    "\n",
    "  y_pred = temp_clf.predict(x_test)\n",
    "  ### End timing\n",
    "\n",
    "  stop = process_time()\n",
    "\n",
    "  # Results\n",
    "  tempDict = {\n",
    "    \"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "    \"Balanced Accuracy\": metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "    \"F1 Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Precision Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Recall Micro\": metrics.recall_score(y_test, y_pred, average='micro'),\n",
    "    \"Runtime\": stop-start,\n",
    "  }\n",
    "  return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModelResults(size, model, results):\n",
    "  def changeKeyValue():\n",
    "    try:\n",
    "      ModelResults[size][model][key] += results[key]\n",
    "    except KeyError:\n",
    "      ModelResults[size][model][key] = results[key]\n",
    "  \n",
    "  # For each metric, attempt to set or add to the value\n",
    "  for key in results:\n",
    "    try:\n",
    "      changeKeyValue()\n",
    "    except KeyError:\n",
    "      ModelResults[size][model] = dict()\n",
    "      changeKeyValue()\n",
    "\n",
    "# Divide each metric by the total number of files tested\n",
    "def findAveragesForModelResults(fileCount):\n",
    "  for size in ModelResults:\n",
    "    for model in ModelResults[size]:\n",
    "      for metric in ModelResults[size][model]:\n",
    "        ModelResults[size][model][metric] /= fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching with test size of 40.0%...\n",
      "b5000d100.csv, b5000d30.csv, b100d10.csv, b1000d10.csv, b1000d100.csv, b100d100.csv, b5000d10.csv, b1000d30.csv, b100d30.csv, "
     ]
    }
   ],
   "source": [
    "OutputResults.clear()\n",
    "ModelResults.clear()\n",
    "\n",
    "for size in TestSize:\n",
    "  print(f\"\\nSearching with test size of {size*100}%...\")\n",
    "  OutputResults[size] = dict()\n",
    "  ModelResults[size] = dict()\n",
    "\n",
    "  for file in FilesToTest:\n",
    "    print(file, end=', ')\n",
    "    OutputResults[size][file] = dict()\n",
    "    currentFileData = NetworkTraffic(file, testSize=size, doNorm=True, doNormAll=False, doTransform=True)\n",
    "    x_train, x_test, y_train, y_test = currentFileData.x_train, currentFileData.x_test, currentFileData.y_train, currentFileData.y_test\n",
    "\n",
    "    for model in ModelsToTest:\n",
    "      #print(f\"{file} : {str(model)}...\")\n",
    "      results = testModel(model, x_train, x_test, y_train, y_test)\n",
    "      OutputResults[size][file].update({str(model): results})\n",
    "      updateModelResults(size, str(model), results)\n",
    "\n",
    "findAveragesForModelResults(len(FilesToTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"EveryFileIndividually_Untuned_AllTestResults.json\", \"w\") as f:\n",
    "  f.write(json.dumps(OutputResults, indent=2))\n",
    "with open(\"EveryFileIndividually_Untuned_ModelResults.json\", \"w\") as f2:\n",
    "  f2.write(json.dumps(ModelResults, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EveryFileIndividual_Untuned_ModelResults.csv\", \"w\") as f3:\n",
    "  f3.write(\"Test Size,Model,Accuracy,Runtime\\n\")\n",
    "  for size in ModelResults:\n",
    "    for model in ModelResults[size]:\n",
    "      f3.write(f\"{size},{model},{ModelResults[size][model]['Accuracy']},{ModelResults[size][model]['Runtime']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
