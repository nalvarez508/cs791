{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods trained on every file, tuned on the rest (untuned)\n",
    "MinMaxScaler is applied to these tests. All models are untuned. The model will be trained on file A, tested on files B,C,D.., then on B, tested on A,C,D.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from modules.NetworkTraffic import NetworkTraffic\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [RandomForestClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(), MLPClassifier(), LinearSVC()]\n",
    "OutputResults = dict()\n",
    "#ModelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, x_train, x_test, y_train, y_test):\n",
    "  print(f\"Testing {str(model)}\", end=', ')\n",
    "  start = process_time()\n",
    "\n",
    "  ### Begin timing\n",
    "  temp_clf = model\n",
    "  temp_clf.fit(x_train, y_train)\n",
    "\n",
    "  y_pred = temp_clf.predict(x_test)\n",
    "  ### End timing\n",
    "\n",
    "  stop = process_time()\n",
    "\n",
    "  # Results\n",
    "  tempDict = {\n",
    "    \"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "    \"Balanced Accuracy\": metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "    \"F1 Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Precision Micro\": metrics.f1_score(y_test, y_pred, average='micro'),\n",
    "    \"Recall Micro\": metrics.recall_score(y_test, y_pred, average='micro'),\n",
    "    \"Runtime\": stop-start,\n",
    "  }\n",
    "  return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModelResults(size, model, results):\n",
    "  def changeKeyValue():\n",
    "    try:\n",
    "      ModelResults[size][model][key] += results[key]\n",
    "    except KeyError:\n",
    "      ModelResults[size][model][key] = results[key]\n",
    "  \n",
    "  # For each metric, attempt to set or add to the value\n",
    "  for key in results:\n",
    "    try:\n",
    "      changeKeyValue()\n",
    "    except KeyError:\n",
    "      ModelResults[size][model] = dict()\n",
    "      changeKeyValue()\n",
    "\n",
    "# Divide each metric by the total number of files tested\n",
    "def findAveragesForModelResults(fileCount):\n",
    "  for size in ModelResults:\n",
    "    for model in ModelResults[size]:\n",
    "      for metric in ModelResults[size][model]:\n",
    "        ModelResults[size][model][metric] /= fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching with test size of 40.0%...\n",
      "b5000d100.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b5000d30.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b100d10.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b1000d10.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b1000d100.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b100d100.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b5000d10.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b1000d30.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), b100d30.csv, \n",
      "Testing RandomForestClassifier(), Testing GradientBoostingClassifier(), Testing DecisionTreeClassifier(), Testing MLPClassifier(), Testing LinearSVC(), "
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "OutputResults.clear()\n",
    "#ModelResults.clear()\n",
    "\n",
    "for size in TestSize:\n",
    "  print(f\"\\nSearching with test size of {size*100}%...\")\n",
    "  OutputResults[size] = dict()\n",
    "  #ModelResults[size] = dict()\n",
    "\n",
    "  for index, file in enumerate(FilesToTest):\n",
    "    print(file, end=', ')\n",
    "    OutputResults[size][file] = dict()\n",
    "    currentFileData = NetworkTraffic(file, testSize=size, doNorm=True, doNormAll=True, doTransform=True)\n",
    "    restOfFiles = deepcopy(FilesToTest)\n",
    "    restOfFiles.pop(index)\n",
    "    restOfTheFilesData = NetworkTraffic(restOfFiles, testSize=size, doNorm=True, doNormAll=True, doTransform=True)\n",
    "    x_train, y_train, x_test, y_test = currentFileData.data, currentFileData.target, restOfTheFilesData.data, restOfTheFilesData.target\n",
    "\n",
    "    for model in ModelsToTest:\n",
    "      #print(f\"{file} : {str(model)}...\")\n",
    "      results = testModel(model, x_train, x_test, y_train, y_test)\n",
    "      OutputResults[size][file].update({str(model): results})\n",
    "      #updateModelResults(size, str(model), results)\n",
    "\n",
    "#findAveragesForModelResults(len(FilesToTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"EveryFileTransfer_Untuned_AllTestResults.json\", \"w\") as f:\n",
    "  f.write(json.dumps(OutputResults, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EveryFileTransfer_Untuned_ModelResults.csv\", \"w\") as f3:\n",
    "  f3.write(\"File Trained On,Model,Accuracy,Runtime\\n\")\n",
    "  for size in OutputResults:\n",
    "    for file in OutputResults[size]:\n",
    "      for model in OutputResults[size][file]:\n",
    "        f3.write(f\"{file},{model},{OutputResults[size][file][model]['Accuracy']},{OutputResults[size][file][model]['Runtime']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
