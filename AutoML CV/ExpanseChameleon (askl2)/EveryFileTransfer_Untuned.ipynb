{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods trained on every file, tested on one at a time (untuned)\n",
    "MinMaxScaler is applied to these tests. All models are untuned. The model will be trained on file A, tested on files B,C,D.., then on B, tested on A,C,D.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir, environ, path\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import numpy as np\n",
    "from modules.NetworkTraffic import NT2\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data/other\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    if file.startswith('b'):\n",
    "      FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [AutoSklearn2Classifier(time_left_for_this_task=60, n_jobs=-1, memory_limit=4096)]\n",
    "OutputResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "theseFiles = dict()\n",
    "for file in FilesToTest:\n",
    "  theseFiles[file] = NT2(file, transform=False, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "df_leader = None\n",
    "\n",
    "def testMe():\n",
    "  global df_leader\n",
    "  df_leader = pd.DataFrame()\n",
    "  OutputResults.clear()\n",
    "  #ModelResults.clear()\n",
    "\n",
    "  for index, file in enumerate(FilesToTest):\n",
    "    print(file, end=' ')\n",
    "    OutputResults[file] = dict()\n",
    "    currentFileData = theseFiles[file]\n",
    "    restOfFiles = deepcopy(FilesToTest)\n",
    "    restOfFiles.pop(index)\n",
    "\n",
    "    for model in ModelsToTest:\n",
    "      model.fit(currentFileData.data, currentFileData.target)\n",
    "      print('[', end=' ')\n",
    "      for file2 in restOfFiles:\n",
    "        print('.', end=' ')\n",
    "        OutputResults[file][file2] = dict()\n",
    "        testFileData = theseFiles[file2]\n",
    "        y_pred = model.predict(testFileData.data)\n",
    "        score = metrics.accuracy_score(testFileData.target, y_pred)\n",
    "        OutputResults[file][file2][\"Accuracy\"] = score\n",
    "        try:\n",
    "          OutputResults[file][file2][\"Final Ensemble\"] = str(model.show_models())\n",
    "        except:\n",
    "          pass\n",
    "        try:\n",
    "          if df_leader.empty: df_leader = model.leaderboard()\n",
    "          else: df_leader = pd.concat([df_leader, model.leaderboard()], ignore_index=False)\n",
    "        except: pass\n",
    "        try:\n",
    "          OutputResults[file][file2][\"Sprint\"] = str(model.sprint_statistics())\n",
    "        except KeyError:\n",
    "          pass\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "\n",
    "def writeMe():\n",
    "  with open(\"EveryFileTransfer_Untuned_AllTestResults.json\", \"a\") as f:\n",
    "    f.write(json.dumps(OutputResults, indent=2))\n",
    "\n",
    "  df_leader.to_csv(\"Leaderboard.csv\", mode='a')\n",
    "\n",
    "  with open(\"EveryFileTransfer_Untuned_ModelResults.csv\", \"a\") as f3:\n",
    "    if path.exists(\"EveryFileTransfer_Untuned_ModelResults.csv\"): f3.write(\"Trained On,Tested On,Model,Accuracy,Runtime\\n\")\n",
    "    for file in OutputResults:\n",
    "      for file2 in OutputResults[file]:\n",
    "        m = str(ModelsToTest[0]).split('(')[0]\n",
    "        f3.write(f\"{file},{file2},{m},{OutputResults[file][file2]['Accuracy']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1000d60.csv [ . ]\n",
      "b1000d35.csv [ . ]\n",
      "b1000d60.csv [ . ]\n",
      "b1000d35.csv [ . ]\n"
     ]
    }
   ],
   "source": [
    "REPEATS = 2\n",
    "for _ in range(0, REPEATS):\n",
    "  testMe()\n",
    "  writeMe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
