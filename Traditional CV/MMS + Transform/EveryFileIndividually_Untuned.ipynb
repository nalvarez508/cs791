{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods tested on every file individually (untuned)\n",
    "MinMaxScaler is applied to these tests. All models are untuned. Results from each model's tests are compiled into a single average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir, environ\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import numpy as np\n",
    "from modules.NetworkTraffic import NT2\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [RandomForestClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(), MLPClassifier(), LinearSVC()]\n",
    "OutputResults = dict()\n",
    "ModelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMe():\n",
    "  OutputResults.clear()\n",
    "  ModelResults.clear()\n",
    "\n",
    "  for size in TestSize:\n",
    "    print(f\"\\nSearching with test size of {size*100}%...\")\n",
    "    OutputResults[size] = dict()\n",
    "    ModelResults[size] = dict()\n",
    "\n",
    "    for file in FilesToTest:\n",
    "      print(file, end=', ')\n",
    "      OutputResults[size][file] = dict()\n",
    "      currentFileData = NT2(file, transform=True)\n",
    "\n",
    "      for model in ModelsToTest:\n",
    "        clf = make_pipeline(MinMaxScaler(), model)\n",
    "        results = model_selection.cross_val_score(clf, currentFileData.data, currentFileData.target, cv=5, n_jobs=-1)\n",
    "        OutputResults[size][file].update({str(model): {\"Accuracy\":np.mean(results), \"Std Dev\":np.std(results)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "def writeMe():\n",
    "  with open(\"EveryFileIndividually_Untuned_AllTestResults.json\", \"a\") as f:\n",
    "    f.write(json.dumps(OutputResults, indent=2))\n",
    "\n",
    "  with open(\"EveryFileIndividual_Untuned_ModelResults.csv\", \"a\") as f3:\n",
    "    if not path.exists(\"EveryFileIndividual_Untuned_ModelResults.csv\"): f3.write(\"File,Model,Accuracy,Std Dev\\n\")\n",
    "    for size in OutputResults:\n",
    "      for file in OutputResults[size]:\n",
    "        for model in OutputResults[size][file]:\n",
    "          f3.write(f\"{file},{model},{OutputResults[size][file][model]['Accuracy']},{OutputResults[size][file][model]['Std Dev']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----0-----\n",
      "\n",
      "Searching with test size of 40.0%...\n",
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, \n",
      "-----1-----\n",
      "\n",
      "Searching with test size of 40.0%...\n",
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, \n",
      "-----2-----\n",
      "\n",
      "Searching with test size of 40.0%...\n",
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, \n",
      "-----3-----\n",
      "\n",
      "Searching with test size of 40.0%...\n",
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, \n",
      "-----4-----\n",
      "\n",
      "Searching with test size of 40.0%...\n",
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, "
     ]
    }
   ],
   "source": [
    "REPEATS = 5\n",
    "for _ in range(0, REPEATS):\n",
    "  print(f\"\\n-----{_}-----\")\n",
    "  testMe()\n",
    "  writeMe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
