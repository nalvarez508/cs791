{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods trained on every file, tuned on the rest (untuned)\n",
    "MinMaxScaler is applied to these tests. All models are untuned. The model will be trained on file A, tested on files B,C,D.., then on B, tested on A,C,D.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from time import process_time\n",
    "from os import listdir, chdir, environ\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "  pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import numpy as np\n",
    "from modules.NetworkTraffic import NT2\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "FilesToTest = list()\n",
    "chdir(\"../../data\")\n",
    "for file in listdir():\n",
    "  if file.endswith(\".csv\"):\n",
    "    FilesToTest.append(file)\n",
    "\n",
    "TestSize = [0.4]\n",
    "ModelsToTest = [RandomForestClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(), MLPClassifier(), LinearSVC()]\n",
    "OutputResults = dict()\n",
    "#ModelResults = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def testMe():\n",
    "  OutputResults.clear()\n",
    "\n",
    "  for index, file in enumerate(FilesToTest):\n",
    "    print(file, end=', ')\n",
    "    OutputResults[file] = dict()\n",
    "    currentFileData = NT2(file, transform=True, drop=True)\n",
    "    restOfFiles = deepcopy(FilesToTest)\n",
    "    restOfFiles.pop(index)\n",
    "    restOfTheFilesData = NT2(restOfFiles, transform=True, drop=True)\n",
    "\n",
    "    for model in ModelsToTest:\n",
    "      #print(f\"{file} : {str(model)}...\")\n",
    "      clf = make_pipeline(MinMaxScaler(), model)\n",
    "      clf.fit(currentFileData.data, currentFileData.target)\n",
    "      y_pred = clf.predict(restOfTheFilesData.data)\n",
    "      score = metrics.accuracy_score(restOfTheFilesData.target, y_pred)\n",
    "      OutputResults[file].update({str(model): {\"Accuracy\": score}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "\n",
    "def writeMe():\n",
    "  with open(\"EveryFileTransfer_Untuned_AllTestResults.json\", \"a\") as f:\n",
    "    f.write(json.dumps(OutputResults, indent=2))\n",
    "\n",
    "  with open(\"EveryFileTransfer_Untuned_ModelResults.csv\", \"a\") as f3:\n",
    "    if not path.exists(\"EveryFileTransfer_Untuned_ModelResults.csv\"): f3.write(\"File Trained On,Model,Accuracy\\n\")\n",
    "    for file in OutputResults:\n",
    "      for model in OutputResults[file]:\n",
    "        f3.write(f\"{file},{model},{OutputResults[file][model]['Accuracy']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, b1000d10.csv, b1000d100.csv, b1000d30.csv, b100d10.csv, b100d100.csv, b100d30.csv, b5000d10.csv, b5000d100.csv, b5000d30.csv, "
     ]
    }
   ],
   "source": [
    "REPEATS = 5\n",
    "for _ in range(0, REPEATS):\n",
    "  testMe()\n",
    "  writeMe()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
